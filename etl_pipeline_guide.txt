# End-to-End ETL Pipeline Implementation Guide for AI Agent

## Project Overview
You are tasked with building an end-to-end ETL (Extract, Transform, Load) pipeline orchestrated by Apache Airflow, running in a Dockerized environment. The pipeline will:
- Extract a CSV file from a GitHub repository.
- Transform the data using Apache Spark.
- Load the transformed data into Apache Hive for storage.
- Ingest metadata into Apache Atlas for data governance.

**Why Apache Hive?**
Apache Hive is chosen as the middle engine because:
- **Native Atlas Integration**: Atlas’s Hive hook automatically captures metadata (e.g., table schemas, lineage) during Hive operations, eliminating the need for custom metadata scripts.
- **Scalability**: Hive supports large-scale data storage and querying on Hadoop HDFS.
- **Spark Compatibility**: Spark integrates seamlessly with Hive via the Hive metastore.
- **Ecosystem Fit**: Hive aligns with Spark and Atlas in the Hadoop ecosystem.

**Prerequisites**
- Docker containers for Airflow, Spark, and Hive are running on the `etl_network` Docker network.
- Access to a GitHub repository containing the CSV file.
- Basic familiarity with Docker, Airflow, Spark, Hive, and Atlas.
- Current date: May 19, 2025.

**Implementation Steps**
Below are detailed steps with tasks, artifacts, and checkpoints to ensure each step is completed before proceeding.

---

### Step 1: Set Up Apache Atlas in Docker
**Objective**: Install and configure Apache Atlas in a Docker container, ensuring integration with Hive and connectivity with other containers.

**Tasks**:
1. Pull a stable Apache Atlas Docker image (e.g., ING Bank’s GitHub image).
2. Configure Atlas with embedded HBase and Solr for metadata storage and search.
3. Connect Atlas to the `etl_network` Docker network.
4. Start Atlas and verify its web UI and Hive connectivity.

**Artifact (docker-compose-atlas.yml)**:
```yaml
version: '3.8'
services:
  atlas:
    image: ghcr.io/ing-bank/apache-atlas:2.2.0
    container_name: atlas
    ports:
      - "21000:21000"
    environment:
      - ATLAS_SERVER_OPTS=-server -Xmx2g
    volumes:
      - atlas_data:/atlas/data
      - atlas_logs:/atlas/logs
    networks:
      - etl_network
volumes:
  atlas_data:
  atlas_logs:
networks:
  etl_network:
    name: etl_network
```

**Checkpoints**:
- Run `docker-compose -f docker-compose-atlas.yml up -d` to start Atlas.
- Verify Atlas UI: Access `http://localhost:21000` and log in (default: admin/admin).
- Confirm network connectivity: Run `docker exec atlas ping hive` from the Atlas container.
- Check Atlas logs: `docker logs atlas` should show no fatal errors and confirm server startup.
- Validate Hive hook readiness: Ensure Hive configuration will include the Atlas hook (see Step 4).

**Notes**:
- Atlas documentation (`http://atlas.apache.org`) recommends embedded HBase/Solr for development.
- Ensure `etl_network` matches the Docker network of Airflow, Spark, and Hive.

---

### Step 2: Configure Airflow DAG
**Objective**: Create an Airflow DAG to orchestrate the ETL pipeline, coordinating CSV extraction, Spark transformation, Hive loading, and Atlas metadata verification.

**Tasks**:
1. Define a Python DAG with tasks for downloading the CSV, submitting a Spark job, loading data to Hive, and verifying Atlas metadata.
2. Ensure Airflow has Spark and Hive dependencies (e.g., Hive JDBC driver).
3. Place the DAG in Airflow’s DAGs folder (`/airflow/dags`).

**Artifact (etl_pipeline.py)**:
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime
import requests
import os

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 5, 19),
    'retries': 1,
}

dag = DAG(
    'etl_pipeline',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
)

def download_csv():
    url = "https://raw.githubusercontent.com/sample/repo/main/data.csv"
    response = requests.get(url)
    with open("/tmp/data.csv", "wb") as f:
        f.write(response.content)

download_task = PythonOperator(
    task_id='download_csv',
    python_callable=download_csv,
    dag=dag,
)

spark_submit = BashOperator(
    task_id='spark_transform',
    bash_command='spark-submit --master local[*] /path/to/spark_script.py',
    dag=dag,
)

load_to_hive = BashOperator(
    task_id='load_to_hive',
    bash_command='echo "Load to Hive placeholder"',
    dag=dag,
)

verify_metadata = BashOperator(
    task_id='verify_metadata_in_atlas',
    bash_command='echo "Verify metadata in Atlas placeholder"',
    dag=dag,
)

download_task >> spark_submit >> load_to_hive >> verify_metadata
```

**Checkpoints**:
- Copy `etl_pipeline.py` to `/airflow/dags`.
- Verify DAG in Airflow UI: Check `http://localhost:8080` for `etl_pipeline`.
- Trigger the DAG manually and confirm `download_csv` task succeeds.
- Check `/tmp/data.csv` exists in the Airflow container.

**Notes**:
- Replace the GitHub URL with the actual CSV file location.
- Ensure Airflow can access Spark and Hive containers via `etl_network`.

---

### Step 3: Transform Data with Spark
**Objective**: Write a Spark script to read the CSV, apply transformations, and save the output as a Parquet file.

**Tasks**:
1. Create a Spark script to read the CSV, filter rows (e.g., `age > 18`), and save as Parquet.
2. Ensure Spark has access to the CSV and Hive metastore.

**Artifact (spark_script.py)**:
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ETLTransform") \
    .config("spark.sql.catalogImplementation", "hive") \
    .config("spark.hadoop.hive.metastore.uris", "thrift://hive:9083") \
    .getOrCreate()

# Read CSV
df = spark.read.csv("/tmp/data.csv", header=True, inferSchema=True)

# Transformation: Filter rows where age > 18
df_transformed = df.filter(df.age > 18)

# Save as Parquet
df_transformed.write.mode("overwrite").parquet("/tmp/transformed_data.parquet")

spark.stop()
```

**Checkpoints**:
- Verify Hive metastore is running: Confirm `thrift://hive:9083` is accessible.
- Copy `spark_script.py` to the Spark container and run: `docker exec spark spark-submit /path/to/spark_script.py`.
- Confirm `/tmp/transformed_data.parquet` exists in the Spark container.
- Check Spark logs for errors.

**Notes**:
- Adjust transformation logic based on CSV schema.
- Use a shared volume for `/tmp` between Airflow and Spark.
- Ensure Hive metastore URI matches Hive container configuration.

---

### Step 4: Load Data into Apache Hive
**Objective**: Load transformed Parquet data into a Hive table and configure the Atlas Hive hook for metadata ingestion.

**Tasks**:
1. Update the Spark script to load Parquet data into a Hive table.
2. Configure Hive to enable the Atlas hook.
3. Create the target Hive table if needed.

**Artifact (spark_script.py - Updated)**:
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ETLTransform") \
    .config("spark.sql.catalogImplementation", "hive") \
    .config("spark.hadoop.hive.metastore.uris", "thrift://hive:9083") \
    .getOrCreate()

# Read CSV
df = spark.read.csv("/tmp/data.csv", header=True, inferSchema=True)

# Transformation
df_transformed = df.filter(df.age > 18)

# Save to Hive
df_transformed.write.mode("overwrite").saveAsTable("default.transformed_data")

spark.stop()
```

**Artifact (hive-site.xml)**:
```xml
<configuration>
  <property>
    <name>atlas.cluster.name</name>
    <value>primary</value>
  </property>
  <property>
    <name>atlas.rest.address</name>
    <value>http://atlas:21000</value>
  </property>
  <property>
    <name>atlas.hook.hive.synchronous</name>
    <value>true</value>
  </property>
</configuration>
```

**Checkpoints**:
- Update Hive configuration: Copy `hive-site.xml` to `/hive/conf` and restart Hive.
- Create Hive table (if needed):
  ```sql
  CREATE TABLE default.transformed_data (
      id INT,
      name STRING,
      age INT
  ) STORED AS PARQUET;
  ```
- Run updated Spark script manually and verify data: `SELECT * FROM default.transformed_data;`.
- Check Atlas UI: Search for `transformed_data` under Hive entities.
- Update Airflow DAG:
  ```python
  load_to_hive = BashOperator(
      task_id='load_to_hive',
      bash_command='spark-submit --master local[*] /path/to/spark_script.py',
      dag=dag,
  )
  ```

**Notes**:
- Adjust Hive table schema based on CSV data.
- Ensure Hive container is accessible from Spark (`hive` hostname).
- Atlas Hive hook documentation: `http://atlas.apache.org/2.2.0/Hook-Hive.html`.

---

### Step 5: Verify Metadata in Apache Atlas
**Objective**: Confirm Hive metadata is ingested into Atlas and accessible via the UI or API.

**Tasks**:
1. Query Atlas to verify Hive table metadata.
2. Add a verification task to the Airflow DAG.

**Artifact (verify_metadata.py)**:
```python
import requests
import json

def verify_metadata():
    atlas_url = "http://atlas:21000/api/atlas/v2/search/basic"
    headers = {"Content-Type": "application/json", "Accept": "application/json"}
    auth = ("admin", "admin")
    query = {
        "typeName": "hive_table",
        "query": "transformed_data",
        "limit": 1
    }
    response = requests.post(atlas_url, json=query, headers=headers, auth=auth)
    if response.status_code == 200 and response.json().get("entities"):
        print("Metadata for 'transformed_data' found in Atlas.")
    else:
        raise Exception("Metadata not found in Atlas.")

if __name__ == "__main__":
    verify_metadata()
```

**Checkpoints**:
- Run `verify_metadata.py` manually: `python verify_metadata.py`.
- Verify Atlas UI: Search for `hive_table` type and `transformed_data` table.
- Update Airflow DAG:
  ```python
  verify_metadata = PythonOperator(
      task_id='verify_metadata_in_atlas',
      python_callable=lambda: __import__('verify_metadata').verify_metadata(),
      dag=dag,
  )
  ```
- Trigger DAG and confirm metadata verification task succeeds.

**Notes**:
- Install `requests` in Airflow: `pip install requests`.
- Atlas REST API: `http://atlas.apache.org/api/v2`.

---

### Step 6: Test the End-to-End Pipeline
**Objective**: Run the complete Airflow DAG and validate the pipeline.

**Tasks**:
1. Trigger the DAG via Airflow UI or CLI.
2. Monitor task logs for successful completion.
3. Validate data in Hive and metadata in Atlas.

**Checkpoints**:
- Confirm CSV download: Check `/tmp/data.csv`.
- Verify Hive data: `SELECT * FROM default.transformed_data;`.
- Check Atlas UI for `transformed_data` metadata (schema, lineage).
- Ensure no errors in Airflow, Spark, Hive, or Atlas logs.

**Notes**:
- Debug failures using logs and network checks.
- Add error handling (e.g., retries) for production.

---

## Additional Guidelines
- **Docker Networking**: Verify containers are on `etl_network` (`docker network inspect etl_network`).
- **Security**: Update default credentials (Atlas: admin/admin, Hive metastore) in production.
- **Scalability**: Optimize Spark jobs and Hive partitioning for large datasets.
- **Monitoring**: Add Airflow notifications (e.g., Slack) for failures.
- **Atlas-Hive Integration**: The Hive hook is the primary metadata ingestion method (`http://atlas.apache.org/2.2.0/Hook-Hive.html`).

## Expected Outcome
The pipeline will:
- Download a CSV from GitHub daily.
- Transform data using Spark.
- Load data into a Hive table.
- Automatically ingest Hive metadata into Atlas.
- Verify metadata in Atlas for governance and lineage.

For issues or clarifications, consult Atlas documentation or provide specific details for further assistance.